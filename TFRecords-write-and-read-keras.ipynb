{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import glob\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "#import skimage.io as io\n",
    "import tensorflow as tf\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def load_image(addr):\n",
    "    # read an image and resize to (224, 224)\n",
    "    # cv2 load images as BGR, convert it to RGB\n",
    "    img = cv2.imread(addr)\n",
    "    if img is None:\n",
    "        return None\n",
    "    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    " \n",
    "def createDataRecord(out_filename, addrs, labels):\n",
    "    # open the TFRecords file\n",
    "    #writer = tf.python_io.TFRecordWriter(out_filename)\n",
    "    options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)\n",
    "    writer = tf.python_io.TFRecordWriter(out_filename, options=options)\n",
    "\n",
    "    for i in range(len(addrs)):\n",
    "        # print how many images are saved every 1000 images\n",
    "        if not i % 1000:\n",
    "            print('Train data: {}/{}'.format(i, len(addrs)))\n",
    "            sys.stdout.flush()\n",
    "        # Load the image\n",
    "        img = load_image(addrs[i])\n",
    "\n",
    "        label = labels[i]\n",
    "\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        # Create a feature\n",
    "        feature = {\n",
    "            'image': _bytes_feature(img.tostring()),\n",
    "            'label': _int64_feature(label)\n",
    "        }\n",
    "        # Create an example protocol buffer\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        \n",
    "        # Serialize to string and write on the file\n",
    "        writer.write(example.SerializeToString())\n",
    "        \n",
    "    writer.close()\n",
    "    sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_addrs: ('dataset-mix\\\\B\\\\VID_20180905_122652621 09_result.jpg', 'dataset-mix\\\\A\\\\VID_20180904_134255333 006_result.jpg', 'dataset-mix\\\\A\\\\VID_20180905_122229488 31_result.jpg', 'dataset-mix\\\\B\\\\VID_20180904_134543427 041_result.jpg', 'dataset-mix\\\\B\\\\VID_20180905_124234130 11_result.jpg', 'dataset-mix\\\\A\\\\VID_20180905_121314144 40_result.jpg', 'dataset-mix\\\\B\\\\VID_20180905_121432570 24_result.jpg', 'dataset-mix\\\\A\\\\VID_20180905_121314144 37_result.jpg', 'dataset-mix\\\\B\\\\VID_20180905_121432570 183_result_result.jpg', 'dataset-mix\\\\A\\\\VID_20180905_125828811 33_result.jpg', 'dataset-mix\\\\A\\\\VID_20180904_134255333 137_result.jpg', 'dataset-mix\\\\A\\\\VID_20180905_121314144 082_result_result.jpg', 'dataset-mix\\\\B\\\\VID_20180905_124234130 33_result.jpg', 'dataset-mix\\\\B\\\\VID_20180904_134543427 179_result.jpg', 'dataset-mix\\\\A\\\\VID_20180904_134255333 016_result.jpg', 'dataset-mix\\\\A\\\\VID_20180904_134255333 148_result.jpg', 'dataset-mix\\\\B\\\\VID_20180905_122652621 23_result.jpg', 'dataset-mix\\\\A\\\\VID_20180905_121314144 01_result.jpg', 'dataset-mix\\\\A\\\\VID_20180905_123902514 40_result.jpg', 'dataset-mix\\\\B\\\\VID_20180904_134543427 042_result.jpg', 'dataset-mix\\\\B\\\\VID_20180904_134543427 079_result.jpg', 'dataset-mix\\\\A\\\\VID_20180905_121314144 143_result.jpg', 'dataset-mix\\\\A\\\\VID_20180904_134255333 003_result.jpg', 'dataset-mix\\\\B\\\\VID_20180904_134543427 246_result.jpg', 'dataset-mix\\\\B\\\\VID_20180905_121432570 21_result.jpg', 'dataset-mix\\\\B\\\\VID_20180905_130354329 25_result.jpg', 'dataset-mix\\\\A\\\\VID_20180905_125828811 49_result.jpg', 'dataset-mix\\\\B\\\\VID_20180904_134543427 002_result.jpg', 'dataset-mix\\\\A\\\\VID_20180905_121314144 101_result_result.jpg', 'dataset-mix\\\\A\\\\VID_20180904_134255333 021_result.jpg', 'dataset-mix\\\\A\\\\VID_20180905_122229488 26_result.jpg', 'dataset-mix\\\\B\\\\VID_20180904_134543427 134_result.jpg', 'dataset-mix\\\\B\\\\VID_20180904_134543427 039_result.jpg', 'dataset-mix\\\\B\\\\VID_20180904_134543427 026_result.jpg', 'dataset-mix\\\\A\\\\VID_20180904_134255333 219_result.jpg', 'dataset-mix\\\\B\\\\VID_20180904_134543427 001_result.jpg', 'dataset-mix\\\\B\\\\VID_20180905_124234130 15_result.jpg', 'dataset-mix\\\\A\\\\VID_20180904_134255333 057_result.jpg', 'dataset-mix\\\\A\\\\VID_20180905_122229488 15_result.jpg', 'dataset-mix\\\\B\\\\VID_20180905_121432570 168_result_result.jpg', 'dataset-mix\\\\A\\\\VID_20180904_134255333 184_result.jpg')\n",
      "train_labels: (1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0)\n",
      "Train data: 0/41\n",
      "Train data: 0/14\n",
      "Train data: 0/14\n"
     ]
    }
   ],
   "source": [
    "cat_dog_train_path = 'dataset-mix/*/*.jpg'\n",
    "# read addresses and labels from the 'train' folder\n",
    "addrs = glob.glob(cat_dog_train_path)\n",
    "labels = [0 if 'A' in addr else 1 for addr in addrs]  # 0 = A, 1 = B\n",
    "\n",
    "# to shuffle data\n",
    "c = list(zip(addrs, labels))\n",
    "shuffle(c)\n",
    "addrs, labels = zip(*c)\n",
    "\n",
    "#print(\"addrs: \" + str(addrs))\n",
    "#print(\"labels: \" + str(labels))\n",
    "\n",
    "# Divide the data into 60% train, 20% validation, and 20% test\n",
    "train_addrs = addrs[0:int(0.6*len(addrs))]\n",
    "train_labels = labels[0:int(0.6*len(labels))]\n",
    "val_addrs = addrs[int(0.6*len(addrs)):int(0.8*len(addrs))]\n",
    "val_labels = labels[int(0.6*len(addrs)):int(0.8*len(addrs))]\n",
    "test_addrs = addrs[int(0.8*len(addrs)):]\n",
    "test_labels = labels[int(0.8*len(labels)):]\n",
    "\n",
    "print(\"train_addrs: \" + str(train_addrs))\n",
    "print(\"train_labels: \" + str(train_labels))\n",
    "\n",
    "createDataRecord('train-zipped.tfrecords', train_addrs, train_labels)\n",
    "createDataRecord('val-zipped.tfrecords', val_addrs, val_labels)\n",
    "createDataRecord('test-zipped.tfrecords', test_addrs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(train_addrs))\n",
    "print(np.shape(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape: (224, 224, 3)\n",
      "label.shape before: (); label: Tensor(\"Cast_1:0\", shape=(), dtype=int32)\n",
      "labels1.shape after expand_dims()\n",
      "dataset: (32,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def parser(record):\n",
    "    keys_to_features = {\n",
    "        \"image\": tf.FixedLenFeature([], tf.string),\n",
    "        \"label\": tf.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    \n",
    "    parsed = tf.parse_single_example(record, keys_to_features)\n",
    "    image = tf.decode_raw(parsed[\"image\"], tf.uint8)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, shape=[224, 224, 3])\n",
    "    label = tf.cast(parsed[\"label\"], tf.int32)\n",
    "    \n",
    "    #return {'image': image}, label\n",
    "    print(\"image.shape: \" + str(np.shape(image)))\n",
    "    print(\"label.shape before: \" + str(np.shape(label)) + \"; label: \" + str(label))\n",
    "\n",
    "    #label = np.array(label)\n",
    "    #print(\"label.shape after making np.array: \" + str(np.shape(label)))\n",
    "    #label = np.expand_dims(label, axis=1)\n",
    "    #label = np.reshape(label, (-1, 1))\n",
    "    print(\"labels1.shape after expand_dims\" + str(np.shape(label)))\n",
    "    \n",
    "    return {'vgg16_input': image}, label\n",
    "\n",
    "\n",
    "def input_fn(filenames=[\"train-zipped.tfrecords\"]):\n",
    "\n",
    "  #To create a TFRecordDataset to read the compressed files:\n",
    "  dataset = tf.data.TFRecordDataset(filenames=filenames, compression_type='GZIP', num_parallel_reads=40)\n",
    "  \n",
    "  dataset = dataset.apply(\n",
    "      tf.contrib.data.shuffle_and_repeat(1024, 1)\n",
    "  )\n",
    "  dataset = dataset.apply(\n",
    "      tf.contrib.data.map_and_batch(parser, 32)\n",
    "  )\n",
    "  #dataset = dataset.map(parser, num_parallel_calls=12)\n",
    "  #dataset = dataset.batch(batch_size=1000)\n",
    "  \n",
    "  #print(\"dataset: \" + str(dataset.make_one_shot_iterator().get_next()))\n",
    "    \n",
    "  dataset = dataset.prefetch(buffer_size=2)\n",
    "  \n",
    "  return dataset\n",
    "\n",
    "\n",
    "def train_input_fn():\n",
    "    return input_fn(filenames=[\"train-zipped.tfrecords\", \"test-zipped.tfrecords\"])\n",
    "\n",
    "def val_input_fn():\n",
    "    return input_fn(filenames=[\"val-zipped.tfrecords\"])\n",
    "\n",
    "dat_set = train_input_fn()\n",
    "item1 = dat_set.make_one_shot_iterator().get_next()[1]\n",
    "res = sess.run(item1)\n",
    "print(\"dataset: \" + str(np.shape(res)))\n",
    "\n",
    "#train_dataset = train_input_fn(\"train-zipped.tfrecords\", \"test-zipped.tfrecords\")\n",
    "#val_dataset = val_input_fn(\"val-zipped.tfrecords\")\n",
    "\n",
    "#print(\"classes: {0};\\n shapes: {1};\\n types: {2}\\n\".format(train_dataset.output_classes, \n",
    "#            train_dataset.output_shapes, train_dataset.output_types))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_keras_estimator(): \n",
    "    from tensorflow.python.keras.applications.vgg16 import VGG16\n",
    "    from tensorflow.python.keras import models\n",
    "    from tensorflow.python.keras import layers\n",
    "    import os\n",
    "\n",
    "    conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(224, 224, 3))\n",
    "    conv_base.trainable = False\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(conv_base)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.RMSprop(lr=2e-5),\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    model_dir = os.path.join(os.getcwd(), \"model-keras-mix\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    print(\"model_dir: \",model_dir)\n",
    "    model.summary()\n",
    "    \n",
    "    print(\"input_names\" + str(model.input_names ))\n",
    "    \n",
    "    model_est = tf.keras.estimator.model_to_estimator(keras_model=model,\n",
    "                                                        model_dir=model_dir)\n",
    "    return model_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dir:  C:\\Users\\Jay\\cs230-code-examples-master\\tensorflow\\model-keras-mix\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 21,137,729\n",
      "Trainable params: 6,423,041\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "input_names['vgg16_input']\n",
      "INFO:tensorflow:Using the Keras model provided.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_id': 0, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_evaluation_master': '', '_master': '', '_tf_random_seed': None, '_save_summary_steps': 100, '_service': None, '_num_worker_replicas': 1, '_task_type': 'worker', '_model_dir': 'C:\\\\Users\\\\Jay\\\\cs230-code-examples-master\\\\tensorflow\\\\model-keras-mix', '_num_ps_replicas': 0, '_log_step_count_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001E3E2661B00>, '_keep_checkpoint_every_n_hours': 10000, '_is_chief': True, '_train_distribute': None, '_device_fn': None, '_session_config': None, '_save_checkpoints_secs': 600, '_global_id_in_cluster': 0}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "image.shape: (224, 224, 3)\n",
      "label.shape before: (); label: Tensor(\"Cast_1:0\", shape=(), dtype=int32, device=/device:CPU:0)\n",
      "labels1.shape after expand_dims()\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "logits and labels must have the same shape ((?, 1) vs (?,))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mmerge_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    669\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_same_rank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m         \u001b[0mnew_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36massert_same_rank\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    714\u001b[0m         raise ValueError(\"Shapes %s and %s must have the same rank\" % (self,\n\u001b[1;32m--> 715\u001b[1;33m                                                                        other))\n\u001b[0m\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes (?,) and (?, 1) must have the same rank",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\u001b[0m in \u001b[0;36msigmoid_cross_entropy_with_logits\u001b[1;34m(_sentinel, labels, logits, name)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m       \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mmerge_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    675\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shapes %s and %s are not compatible\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes (?,) and (?, 1) are not compatible",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-ea6455254afe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_keras\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--- %s seconds ---\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(estimator, train_spec, eval_spec)\u001b[0m\n\u001b[0;32m    445\u001b[0m         '(with task id 0).  Given task id {}'.format(config.task_id))\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    529\u001b[0m         config.task_type != run_config_lib.TaskType.EVALUATOR):\n\u001b[0;32m    530\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Running training and evaluation locally (non-distributed).'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;31m# Distributed case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\training.py\u001b[0m in \u001b[0;36mrun_local\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    667\u001b[0m           \u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m           \u001b[0mmax_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m           hooks=train_hooks)\n\u001b[0m\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_continuous_eval_listener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1117\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1119\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[1;32m-> 1132\u001b[1;33m           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[0;32m   1133\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[0;32m   1134\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[1;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Calling model_fn.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1107\u001b[1;33m     \u001b[0mmodel_fn_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1108\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done calling model_fn.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\keras.py\u001b[0m in \u001b[0;36mmodel_fn\u001b[1;34m(features, labels, mode)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;34m\"\"\"model_fn for keras Estimator.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m     model = _clone_and_build_model(mode, keras_model, custom_objects, features,\n\u001b[1;32m--> 344\u001b[1;33m                                    labels)\n\u001b[0m\u001b[0;32m    345\u001b[0m     \u001b[0mmodel_output_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[1;31m# We need to make sure that the output names of the last layer in the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\keras.py\u001b[0m in \u001b[0;36m_clone_and_build_model\u001b[1;34m(mode, keras_model, custom_objects, features, labels)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0msample_weight_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeras_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_weight_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[0mweighted_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeras_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweighted_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m         target_tensors=target_tensors)\n\u001b[0m\u001b[0;32m    327\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[0mloss_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_weights_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m           \u001b[0moutput_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweighted_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_tensors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mweighted\u001b[1;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[0;32m    435\u001b[0m     \"\"\"\n\u001b[0;32m    436\u001b[0m     \u001b[1;31m# score_array has ndim >= 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m     \u001b[0mscore_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m       \u001b[1;31m# Cast the mask to floatX to avoid float64 upcasting in theano\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\losses.py\u001b[0m in \u001b[0;36mbinary_crossentropy\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m    130\u001b[0m            'keras.losses.binary_crossentropy')\n\u001b[0;32m    131\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbinary_crossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_crossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbinary_crossentropy\u001b[1;34m(target, output, from_logits)\u001b[0m\n\u001b[0;32m   3521\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclip_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mepsilon_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3522\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3523\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\u001b[0m in \u001b[0;36msigmoid_cross_entropy_with_logits\u001b[1;34m(_sentinel, labels, logits, name)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m       raise ValueError(\"logits and labels must have the same shape (%s vs %s)\" %\n\u001b[1;32m--> 166\u001b[1;33m                        (logits.get_shape(), labels.get_shape()))\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;31m# The logistic loss formula from above is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: logits and labels must have the same shape ((?, 1) vs (?,))"
     ]
    }
   ],
   "source": [
    "\n",
    "model_keras = make_keras_estimator()\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=10)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=val_input_fn)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "tf.estimator.train_and_evaluate(model_keras, train_spec, eval_spec)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
